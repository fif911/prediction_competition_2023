{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:26.075886Z",
     "start_time": "2024-04-14T15:25:26.071715Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:28.763494Z",
     "start_time": "2024-04-14T15:25:26.077793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ngboost import NGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Load Boston housing dataset\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "Y = raw_df.values[1::2, 2]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "ngb = NGBRegressor().fit(X_train, Y_train)\n",
    "Y_preds = ngb.predict(X_test)\n",
    "Y_dists = ngb.pred_dist(X_test)\n",
    "\n",
    "# test Mean Squared Error\n",
    "test_MSE_ngb = mean_squared_error(Y_preds, Y_test)\n",
    "print('NGB Test MSE', test_MSE_ngb)\n",
    "\n",
    "# test Negative Log Likelihood\n",
    "test_NLL = -Y_dists.logpdf(Y_test).mean()\n",
    "print('Test NLL', test_NLL)\n"
   ],
   "id": "ba17aab5e56c713",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=3.6201 val_loss=0.0000 scale=1.0000 norm=6.6257\n",
      "[iter 100] loss=2.6889 val_loss=0.0000 scale=2.0000 norm=4.9218\n",
      "[iter 200] loss=2.1723 val_loss=0.0000 scale=2.0000 norm=3.5271\n",
      "[iter 300] loss=1.9476 val_loss=0.0000 scale=1.0000 norm=1.5561\n",
      "[iter 400] loss=1.8292 val_loss=0.0000 scale=1.0000 norm=1.4508\n",
      "NGB Test MSE 6.827682147188644\n",
      "Test NLL 2.6578473680711956\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:28.765749Z",
     "start_time": "2024-04-14T15:25:28.764218Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c47ebc81833a8482",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:28.769127Z",
     "start_time": "2024-04-14T15:25:28.766501Z"
    }
   },
   "cell_type": "code",
   "source": "len(Y_preds)",
   "id": "dabaa0e5e83a2cad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:28.776320Z",
     "start_time": "2024-04-14T15:25:28.770890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sampled = Y_dists.sample(100)[0]\n",
    "sampled"
   ],
   "id": "4cd35c920a2c1b17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.50390912, 33.32173509,  3.0963037 , 21.49716915, 22.66030202,\n",
       "       20.8567441 , 21.43548531, 23.23202855, 18.58570453, 26.97337897,\n",
       "       43.86710242, 22.44779653, 17.13503729, 15.91384534, 12.99446203,\n",
       "       18.36826907, 14.5383668 , 25.41509722, 22.84738445, 21.80432621,\n",
       "       13.33281677, 19.89283658, 24.67257091, 16.01727641, 30.17506929,\n",
       "       46.01042869, 32.5910251 ,  9.17509812, 19.34270122, 24.06737161,\n",
       "       27.58436577,  6.64513659, 22.01040989, 16.69587509, 11.46778994,\n",
       "       13.95470353, 27.03159858, 21.97255638, 21.27718124, 46.46988944,\n",
       "       11.58567468, 12.18059409, 23.69188399, 13.9475079 , 34.47478982,\n",
       "       15.00346177, 25.42687038, 18.79829981, 16.08299193, 27.97376077,\n",
       "       25.52211584, 43.17855874, 22.41936694, 18.76621799, 23.05584792,\n",
       "       13.61872621, 32.72460295, 19.70852131, 22.28829392, 36.72673306,\n",
       "       13.88859415, 23.38702576, 43.67180902, 29.27343839, 21.51797544,\n",
       "       33.18224936, 19.20168212,  9.00591471, 18.48452065, 42.57722313,\n",
       "       31.27133531, 19.92718307, 26.2250357 , 24.40268335, 21.25991581,\n",
       "       22.19891775, 28.8957211 ,  8.01306565, 27.0185545 , 27.0561873 ,\n",
       "       15.77295431, 16.75279722, 23.64812404, 36.57141029, 25.58989094,\n",
       "       17.88473036, 43.37241393, 47.18373282,  8.73152278, 13.12511338,\n",
       "       15.4583936 , 20.69763962, 22.62887105, 18.18138611, 20.9902306 ,\n",
       "       25.75729016, 20.55526681, 11.89868693, 20.43692024, 21.17047368,\n",
       "       16.24700154, 20.30729848])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:28.838118Z",
     "start_time": "2024-04-14T15:25:28.776928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot hist for sampled\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sampled, bins=20)\n",
    "plt.show()"
   ],
   "id": "15f2ac4c0db55cd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:25:54.522933Z",
     "start_time": "2024-04-14T15:25:28.839068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setting up parameter grid for XGBoost fine-tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best estimator\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Predicting with the best estimator\n",
    "Y_xgb_preds = best_xgb.predict(X_test)\n",
    "\n",
    "# test Mean Squared Error\n",
    "test_MSE_ngb = mean_squared_error(Y_preds, Y_test)\n",
    "print('NGB Test MSE', test_MSE_ngb)\n",
    "\n",
    "# Test Mean Squared Error for XGBoost\n",
    "test_MSE_xgb = mean_squared_error(Y_xgb_preds, Y_test)\n",
    "print('XGBoost Best Test MSE:', test_MSE_xgb)\n",
    "print('Best parameters:', grid_search.best_params_)"
   ],
   "id": "91d8c2e9835ba086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "NGB Test MSE 6.827682147188644\n",
      "XGBoost Best Test MSE: 6.271459637863312\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T15:48:52.276774Z",
     "start_time": "2024-04-14T15:46:28.667077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from ngboost.distns import Normal\n",
    "from ngboost.scores import LogScore\n",
    "\n",
    "# Setting up parameter grid for NGBoost fine-tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'minibatch_frac': [1.0, 0.5],\n",
    "    'col_sample': [1.0, 0.8],\n",
    "}\n",
    "\n",
    "# Initialize the NGBoost regressor\n",
    "ngb = NGBRegressor()\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search_ngb = GridSearchCV(\n",
    "    estimator=ngb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_ngb.fit(X_train, Y_train)\n",
    "\n",
    "# Best estimator\n",
    "best_ngb = grid_search_ngb.best_estimator_\n",
    "\n",
    "# Predicting with the best estimator\n",
    "Y_ngb_preds = best_ngb.predict(X_test)\n",
    "\n",
    "# Test Mean Squared Error for NGBoost\n",
    "test_MSE_ngb = mean_squared_error(Y_ngb_preds, Y_test)\n",
    "print('NGBoost Best Test MSE:', test_MSE_ngb)\n",
    "print('Best parameters:', grid_search_ngb.best_params_)"
   ],
   "id": "49ff5e4649923cce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=3.1439 val_loss=0.0000 scale=2.0000 norm=8.4159\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=3.0937 val_loss=0.0000 scale=2.0000 norm=7.6785\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=3.1235 val_loss=0.0000 scale=2.0000 norm=8.0532\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=3.1219 val_loss=0.0000 scale=2.0000 norm=8.1910\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=3.0674 val_loss=0.0000 scale=2.0000 norm=7.4053\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=3.1439 val_loss=0.0000 scale=2.0000 norm=8.4159\n",
      "[iter 200] loss=2.7471 val_loss=0.0000 scale=2.0000 norm=5.2482\n",
      "[iter 300] loss=2.4260 val_loss=0.0000 scale=2.0000 norm=4.0674\n",
      "[iter 400] loss=2.1809 val_loss=0.0000 scale=1.0000 norm=1.7567\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=3.0937 val_loss=0.0000 scale=2.0000 norm=7.6785\n",
      "[iter 200] loss=2.6970 val_loss=0.0000 scale=2.0000 norm=4.8179\n",
      "[iter 300] loss=2.3738 val_loss=0.0000 scale=2.0000 norm=3.7366\n",
      "[iter 400] loss=2.1219 val_loss=0.0000 scale=2.0000 norm=3.2296\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=3.1235 val_loss=0.0000 scale=2.0000 norm=8.0532\n",
      "[iter 200] loss=2.7305 val_loss=0.0000 scale=2.0000 norm=5.0423\n",
      "[iter 300] loss=2.4132 val_loss=0.0000 scale=2.0000 norm=3.9753\n",
      "[iter 400] loss=2.1688 val_loss=0.0000 scale=2.0000 norm=3.4538\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=3.1219 val_loss=0.0000 scale=2.0000 norm=8.1910\n",
      "[iter 200] loss=2.7284 val_loss=0.0000 scale=2.0000 norm=5.1413\n",
      "[iter 300] loss=2.4044 val_loss=0.0000 scale=2.0000 norm=4.0151\n",
      "[iter 400] loss=2.1504 val_loss=0.0000 scale=2.0000 norm=3.4262\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=3.0674 val_loss=0.0000 scale=2.0000 norm=7.4053\n",
      "[iter 200] loss=2.6821 val_loss=0.0000 scale=2.0000 norm=4.8155\n",
      "[iter 300] loss=2.3687 val_loss=0.0000 scale=2.0000 norm=3.8635\n",
      "[iter 400] loss=2.1293 val_loss=0.0000 scale=2.0000 norm=3.3701\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=3.0710 val_loss=0.0000 scale=2.0000 norm=7.7072\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=3.0771 val_loss=0.0000 scale=2.0000 norm=7.6166\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=3.1046 val_loss=0.0000 scale=2.0000 norm=7.9514\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=3.0451 val_loss=0.0000 scale=2.0000 norm=7.3141\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=3.0796 val_loss=0.0000 scale=2.0000 norm=7.6571\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=3.0687 val_loss=0.0000 scale=2.0000 norm=7.7017\n",
      "[iter 200] loss=2.6890 val_loss=0.0000 scale=2.0000 norm=4.9863\n",
      "[iter 300] loss=2.4302 val_loss=0.0000 scale=2.0000 norm=4.3869\n",
      "[iter 400] loss=2.2264 val_loss=0.0000 scale=2.0000 norm=3.8199\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=3.0677 val_loss=0.0000 scale=2.0000 norm=7.5042\n",
      "[iter 200] loss=2.6605 val_loss=0.0000 scale=2.0000 norm=4.5888\n",
      "[iter 300] loss=2.3895 val_loss=0.0000 scale=1.0000 norm=2.0257\n",
      "[iter 400] loss=2.1786 val_loss=0.0000 scale=1.0000 norm=1.7802\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=3.1016 val_loss=0.0000 scale=2.0000 norm=7.9129\n",
      "[iter 200] loss=2.7052 val_loss=0.0000 scale=2.0000 norm=5.0936\n",
      "[iter 300] loss=2.4346 val_loss=0.0000 scale=2.0000 norm=4.3208\n",
      "[iter 400] loss=2.2259 val_loss=0.0000 scale=2.0000 norm=3.7762\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=3.0422 val_loss=0.0000 scale=2.0000 norm=7.2810\n",
      "[iter 200] loss=2.6796 val_loss=0.0000 scale=2.0000 norm=5.1077\n",
      "[iter 300] loss=2.4210 val_loss=0.0000 scale=2.0000 norm=4.2963\n",
      "[iter 400] loss=2.1805 val_loss=0.0000 scale=2.0000 norm=3.5587\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=3.0759 val_loss=0.0000 scale=2.0000 norm=7.6173\n",
      "[iter 200] loss=2.6729 val_loss=0.0000 scale=2.0000 norm=4.7457\n",
      "[iter 300] loss=2.3793 val_loss=0.0000 scale=2.0000 norm=4.1345\n",
      "[iter 400] loss=2.1702 val_loss=0.0000 scale=2.0000 norm=3.5927\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=2.7411 val_loss=0.0000 scale=2.0000 norm=5.2013\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=2.6972 val_loss=0.0000 scale=2.0000 norm=4.8107\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=2.7254 val_loss=0.0000 scale=2.0000 norm=5.0092\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=2.7204 val_loss=0.0000 scale=2.0000 norm=5.0915\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=2.6891 val_loss=0.0000 scale=2.0000 norm=4.8377\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=2.7411 val_loss=0.0000 scale=2.0000 norm=5.2013\n",
      "[iter 200] loss=2.1787 val_loss=0.0000 scale=1.0000 norm=1.7515\n",
      "[iter 300] loss=1.9093 val_loss=0.0000 scale=1.0000 norm=1.5026\n",
      "[iter 400] loss=1.7615 val_loss=0.0000 scale=2.0000 norm=2.7442\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=2.6972 val_loss=0.0000 scale=2.0000 norm=4.8107\n",
      "[iter 200] loss=2.1206 val_loss=0.0000 scale=2.0000 norm=3.2322\n",
      "[iter 300] loss=1.8353 val_loss=0.0000 scale=2.0000 norm=2.8193\n",
      "[iter 400] loss=1.6980 val_loss=0.0000 scale=1.0000 norm=1.2998\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=2.7254 val_loss=0.0000 scale=2.0000 norm=5.0092\n",
      "[iter 200] loss=2.1635 val_loss=0.0000 scale=2.0000 norm=3.4375\n",
      "[iter 300] loss=1.8958 val_loss=0.0000 scale=2.0000 norm=2.9821\n",
      "[iter 400] loss=1.7669 val_loss=0.0000 scale=1.0000 norm=1.3826\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=2.7204 val_loss=0.0000 scale=2.0000 norm=5.0915\n",
      "[iter 200] loss=2.1475 val_loss=0.0000 scale=2.0000 norm=3.4222\n",
      "[iter 300] loss=1.8658 val_loss=0.0000 scale=2.0000 norm=2.8717\n",
      "[iter 400] loss=1.7186 val_loss=0.0000 scale=1.0000 norm=1.3145\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=2.6891 val_loss=0.0000 scale=2.0000 norm=4.8377\n",
      "[iter 200] loss=2.1370 val_loss=0.0000 scale=2.0000 norm=3.3879\n",
      "[iter 300] loss=1.8783 val_loss=0.0000 scale=1.0000 norm=1.4687\n",
      "[iter 400] loss=1.7377 val_loss=0.0000 scale=1.0000 norm=1.3436\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=2.6835 val_loss=0.0000 scale=2.0000 norm=4.8843\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=2.6858 val_loss=0.0000 scale=2.0000 norm=4.7805\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=2.7339 val_loss=0.0000 scale=2.0000 norm=5.2652\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=2.6738 val_loss=0.0000 scale=2.0000 norm=4.9996\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=2.7038 val_loss=0.0000 scale=2.0000 norm=5.0776\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=2.6836 val_loss=0.0000 scale=2.0000 norm=4.8937\n",
      "[iter 200] loss=2.1910 val_loss=0.0000 scale=1.0000 norm=1.8363\n",
      "[iter 300] loss=1.9861 val_loss=0.0000 scale=1.0000 norm=1.6740\n",
      "[iter 400] loss=1.8465 val_loss=0.0000 scale=2.0000 norm=3.0751\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=2.6843 val_loss=0.0000 scale=2.0000 norm=4.7719\n",
      "[iter 200] loss=2.1532 val_loss=0.0000 scale=2.0000 norm=3.4278\n",
      "[iter 300] loss=1.9286 val_loss=0.0000 scale=1.0000 norm=1.5470\n",
      "[iter 400] loss=1.7876 val_loss=0.0000 scale=2.0000 norm=2.8559\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=2.7405 val_loss=0.0000 scale=2.0000 norm=5.3157\n",
      "[iter 200] loss=2.2440 val_loss=0.0000 scale=2.0000 norm=3.8713\n",
      "[iter 300] loss=2.0128 val_loss=0.0000 scale=1.0000 norm=1.6896\n",
      "[iter 400] loss=1.8549 val_loss=0.0000 scale=1.0000 norm=1.5328\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=2.6788 val_loss=0.0000 scale=2.0000 norm=5.0480\n",
      "[iter 200] loss=2.1998 val_loss=0.0000 scale=1.0000 norm=1.8476\n",
      "[iter 300] loss=1.9873 val_loss=0.0000 scale=1.0000 norm=1.6309\n",
      "[iter 400] loss=1.7943 val_loss=0.0000 scale=1.0000 norm=1.4278\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=2.7025 val_loss=0.0000 scale=2.0000 norm=5.0577\n",
      "[iter 200] loss=2.1535 val_loss=0.0000 scale=2.0000 norm=3.4758\n",
      "[iter 300] loss=1.9662 val_loss=0.0000 scale=1.0000 norm=1.6368\n",
      "[iter 400] loss=1.8235 val_loss=0.0000 scale=1.0000 norm=1.4440\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=1.6337 val_loss=0.0000 scale=1.0000 norm=1.2588\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=1.5733 val_loss=0.0000 scale=1.0000 norm=1.1957\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=1.6260 val_loss=0.0000 scale=1.0000 norm=1.2645\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=1.6106 val_loss=0.0000 scale=1.0000 norm=1.2233\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=1.5731 val_loss=0.0000 scale=1.0000 norm=1.2038\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=1.6337 val_loss=0.0000 scale=1.0000 norm=1.2588\n",
      "[iter 200] loss=1.2403 val_loss=0.0000 scale=1.0000 norm=0.9698\n",
      "[iter 300] loss=0.9434 val_loss=0.0000 scale=1.0000 norm=0.8018\n",
      "[iter 400] loss=0.7120 val_loss=0.0000 scale=0.5000 norm=0.3463\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=1.5733 val_loss=0.0000 scale=1.0000 norm=1.1957\n",
      "[iter 200] loss=1.2073 val_loss=0.0000 scale=1.0000 norm=0.9361\n",
      "[iter 300] loss=0.9537 val_loss=0.0000 scale=0.5000 norm=0.3971\n",
      "[iter 400] loss=0.7659 val_loss=0.0000 scale=0.5000 norm=0.3552\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=1.6260 val_loss=0.0000 scale=1.0000 norm=1.2645\n",
      "[iter 200] loss=1.2483 val_loss=0.0000 scale=1.0000 norm=0.9688\n",
      "[iter 300] loss=0.9699 val_loss=0.0000 scale=1.0000 norm=0.8034\n",
      "[iter 400] loss=0.7299 val_loss=0.0000 scale=1.0000 norm=0.6885\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=1.6106 val_loss=0.0000 scale=1.0000 norm=1.2233\n",
      "[iter 200] loss=1.2317 val_loss=0.0000 scale=1.0000 norm=0.9408\n",
      "[iter 300] loss=0.9000 val_loss=0.0000 scale=1.0000 norm=0.7553\n",
      "[iter 400] loss=0.6475 val_loss=0.0000 scale=1.0000 norm=0.6442\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=1.5731 val_loss=0.0000 scale=1.0000 norm=1.2038\n",
      "[iter 200] loss=1.1823 val_loss=0.0000 scale=1.0000 norm=0.9283\n",
      "[iter 300] loss=0.9321 val_loss=0.0000 scale=1.0000 norm=0.7904\n",
      "[iter 400] loss=0.7831 val_loss=0.0000 scale=0.0039 norm=0.0028\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=1.7355 val_loss=0.0000 scale=1.0000 norm=1.3981\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=1.6839 val_loss=0.0000 scale=1.0000 norm=1.3194\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=1.7898 val_loss=0.0000 scale=1.0000 norm=1.5051\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=1.7383 val_loss=0.0000 scale=1.0000 norm=1.4340\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=1.7436 val_loss=0.0000 scale=1.0000 norm=1.4003\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=1.7521 val_loss=0.0000 scale=1.0000 norm=1.4145\n",
      "[iter 200] loss=1.4383 val_loss=0.0000 scale=1.0000 norm=1.1494\n",
      "[iter 300] loss=1.0726 val_loss=0.0000 scale=1.0000 norm=0.8808\n",
      "[iter 400] loss=0.8484 val_loss=0.0000 scale=0.5000 norm=0.3705\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=1.7055 val_loss=0.0000 scale=1.0000 norm=1.3508\n",
      "[iter 200] loss=1.3802 val_loss=0.0000 scale=1.0000 norm=1.0640\n",
      "[iter 300] loss=1.0831 val_loss=0.0000 scale=1.0000 norm=0.8558\n",
      "[iter 400] loss=0.8149 val_loss=0.0000 scale=1.0000 norm=0.6969\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=1.8096 val_loss=0.0000 scale=1.0000 norm=1.5132\n",
      "[iter 200] loss=1.5267 val_loss=0.0000 scale=1.0000 norm=1.2482\n",
      "[iter 300] loss=1.1659 val_loss=0.0000 scale=1.0000 norm=0.9222\n",
      "[iter 400] loss=0.8911 val_loss=0.0000 scale=1.0000 norm=0.7796\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=1.7929 val_loss=0.0000 scale=2.0000 norm=2.9890\n",
      "[iter 200] loss=1.4959 val_loss=0.0000 scale=1.0000 norm=1.1777\n",
      "[iter 300] loss=1.1244 val_loss=0.0000 scale=1.0000 norm=0.8935\n",
      "[iter 400] loss=0.8485 val_loss=0.0000 scale=1.0000 norm=0.7043\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=1.7502 val_loss=0.0000 scale=1.0000 norm=1.4116\n",
      "[iter 200] loss=1.3160 val_loss=0.0000 scale=1.0000 norm=1.0083\n",
      "[iter 300] loss=1.1064 val_loss=0.0000 scale=1.0000 norm=0.8808\n",
      "[iter 400] loss=0.8165 val_loss=0.0000 scale=1.0000 norm=0.7109\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=3.1440 val_loss=0.0000 scale=1.0000 norm=4.2516\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=3.0868 val_loss=0.0000 scale=2.0000 norm=7.6511\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=3.0900 val_loss=0.0000 scale=2.0000 norm=7.7077\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=3.0833 val_loss=0.0000 scale=2.0000 norm=7.8050\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=3.0686 val_loss=0.0000 scale=2.0000 norm=7.4421\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=3.1440 val_loss=0.0000 scale=1.0000 norm=4.2516\n",
      "[iter 200] loss=2.7635 val_loss=0.0000 scale=2.0000 norm=5.3698\n",
      "[iter 300] loss=2.4415 val_loss=0.0000 scale=2.0000 norm=4.1329\n",
      "[iter 400] loss=2.1881 val_loss=0.0000 scale=2.0000 norm=3.5381\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=3.0868 val_loss=0.0000 scale=2.0000 norm=7.6511\n",
      "[iter 200] loss=2.6915 val_loss=0.0000 scale=2.0000 norm=4.8242\n",
      "[iter 300] loss=2.3636 val_loss=0.0000 scale=2.0000 norm=3.7341\n",
      "[iter 400] loss=2.1112 val_loss=0.0000 scale=2.0000 norm=3.2290\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=3.0900 val_loss=0.0000 scale=2.0000 norm=7.7077\n",
      "[iter 200] loss=2.7046 val_loss=0.0000 scale=2.0000 norm=4.9458\n",
      "[iter 300] loss=2.3912 val_loss=0.0000 scale=2.0000 norm=3.9320\n",
      "[iter 400] loss=2.1525 val_loss=0.0000 scale=2.0000 norm=3.4506\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=3.0833 val_loss=0.0000 scale=2.0000 norm=7.8050\n",
      "[iter 200] loss=2.6992 val_loss=0.0000 scale=2.0000 norm=4.9984\n",
      "[iter 300] loss=2.3808 val_loss=0.0000 scale=2.0000 norm=3.9430\n",
      "[iter 400] loss=2.1390 val_loss=0.0000 scale=2.0000 norm=3.4076\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=3.0686 val_loss=0.0000 scale=2.0000 norm=7.4421\n",
      "[iter 200] loss=2.6850 val_loss=0.0000 scale=2.0000 norm=4.8526\n",
      "[iter 300] loss=2.3710 val_loss=0.0000 scale=2.0000 norm=3.8862\n",
      "[iter 400] loss=2.1298 val_loss=0.0000 scale=2.0000 norm=3.3872\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=3.0840 val_loss=0.0000 scale=2.0000 norm=7.7386\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=3.0704 val_loss=0.0000 scale=2.0000 norm=7.4660\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=3.0636 val_loss=0.0000 scale=2.0000 norm=7.1504\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=3.0369 val_loss=0.0000 scale=2.0000 norm=7.2549\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=3.1437 val_loss=0.0000 scale=2.0000 norm=8.8844\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=3.0815 val_loss=0.0000 scale=2.0000 norm=7.6938\n",
      "[iter 200] loss=2.6707 val_loss=0.0000 scale=2.0000 norm=4.6639\n",
      "[iter 300] loss=2.4558 val_loss=0.0000 scale=2.0000 norm=4.4538\n",
      "[iter 400] loss=2.2487 val_loss=0.0000 scale=2.0000 norm=3.9444\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=3.0707 val_loss=0.0000 scale=2.0000 norm=7.4653\n",
      "[iter 200] loss=2.6824 val_loss=0.0000 scale=2.0000 norm=4.6769\n",
      "[iter 300] loss=2.4215 val_loss=0.0000 scale=2.0000 norm=4.2092\n",
      "[iter 400] loss=2.1659 val_loss=0.0000 scale=2.0000 norm=3.4893\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=3.0632 val_loss=0.0000 scale=2.0000 norm=7.1482\n",
      "[iter 200] loss=2.6926 val_loss=0.0000 scale=2.0000 norm=4.6804\n",
      "[iter 300] loss=2.4260 val_loss=0.0000 scale=2.0000 norm=4.2046\n",
      "[iter 400] loss=2.2008 val_loss=0.0000 scale=2.0000 norm=3.5952\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=3.0394 val_loss=0.0000 scale=2.0000 norm=7.2791\n",
      "[iter 200] loss=2.6829 val_loss=0.0000 scale=2.0000 norm=4.9657\n",
      "[iter 300] loss=2.3893 val_loss=0.0000 scale=2.0000 norm=4.0066\n",
      "[iter 400] loss=2.1513 val_loss=0.0000 scale=2.0000 norm=3.3273\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=3.1319 val_loss=0.0000 scale=2.0000 norm=8.7562\n",
      "[iter 200] loss=2.6960 val_loss=0.0000 scale=2.0000 norm=4.9783\n",
      "[iter 300] loss=2.3932 val_loss=0.0000 scale=2.0000 norm=4.1132\n",
      "[iter 400] loss=2.1729 val_loss=0.0000 scale=2.0000 norm=3.5871\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=2.7563 val_loss=0.0000 scale=2.0000 norm=5.3252\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=2.6750 val_loss=0.0000 scale=2.0000 norm=4.7338\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=2.7205 val_loss=0.0000 scale=2.0000 norm=5.0187\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=2.6981 val_loss=0.0000 scale=2.0000 norm=4.9963\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=2.6945 val_loss=0.0000 scale=2.0000 norm=4.9063\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=2.7563 val_loss=0.0000 scale=2.0000 norm=5.3252\n",
      "[iter 200] loss=2.1876 val_loss=0.0000 scale=2.0000 norm=3.5373\n",
      "[iter 300] loss=1.9032 val_loss=0.0000 scale=1.0000 norm=1.5008\n",
      "[iter 400] loss=1.7567 val_loss=0.0000 scale=2.0000 norm=2.7419\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=2.6750 val_loss=0.0000 scale=2.0000 norm=4.7338\n",
      "[iter 200] loss=2.0923 val_loss=0.0000 scale=2.0000 norm=3.1844\n",
      "[iter 300] loss=1.8066 val_loss=0.0000 scale=1.0000 norm=1.3781\n",
      "[iter 400] loss=1.6674 val_loss=0.0000 scale=1.0000 norm=1.2703\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=2.7205 val_loss=0.0000 scale=2.0000 norm=5.0187\n",
      "[iter 200] loss=2.1728 val_loss=0.0000 scale=2.0000 norm=3.5015\n",
      "[iter 300] loss=1.9099 val_loss=0.0000 scale=1.0000 norm=1.5163\n",
      "[iter 400] loss=1.7827 val_loss=0.0000 scale=1.0000 norm=1.4048\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=2.6981 val_loss=0.0000 scale=2.0000 norm=4.9963\n",
      "[iter 200] loss=2.1350 val_loss=0.0000 scale=2.0000 norm=3.4163\n",
      "[iter 300] loss=1.8694 val_loss=0.0000 scale=1.0000 norm=1.4506\n",
      "[iter 400] loss=1.7188 val_loss=0.0000 scale=1.0000 norm=1.3155\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=2.6945 val_loss=0.0000 scale=2.0000 norm=4.9063\n",
      "[iter 200] loss=2.1394 val_loss=0.0000 scale=2.0000 norm=3.4168\n",
      "[iter 300] loss=1.8638 val_loss=0.0000 scale=2.0000 norm=2.9443\n",
      "[iter 400] loss=1.7265 val_loss=0.0000 scale=2.0000 norm=2.6921\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=2.7207 val_loss=0.0000 scale=2.0000 norm=5.3006\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=2.6974 val_loss=0.0000 scale=2.0000 norm=4.8998\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=2.6839 val_loss=0.0000 scale=2.0000 norm=4.7325\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=2.6700 val_loss=0.0000 scale=2.0000 norm=4.8045\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=2.7667 val_loss=0.0000 scale=2.0000 norm=5.8476\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=2.7145 val_loss=0.0000 scale=2.0000 norm=5.2762\n",
      "[iter 200] loss=2.1950 val_loss=0.0000 scale=2.0000 norm=3.5014\n",
      "[iter 300] loss=2.0519 val_loss=0.0000 scale=2.0000 norm=3.5569\n",
      "[iter 400] loss=1.8843 val_loss=0.0000 scale=2.0000 norm=3.1156\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=2.6895 val_loss=0.0000 scale=2.0000 norm=4.8703\n",
      "[iter 200] loss=2.1639 val_loss=0.0000 scale=2.0000 norm=3.4138\n",
      "[iter 300] loss=1.9638 val_loss=0.0000 scale=2.0000 norm=3.1929\n",
      "[iter 400] loss=1.7758 val_loss=0.0000 scale=1.0000 norm=1.3928\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=2.6838 val_loss=0.0000 scale=2.0000 norm=4.7189\n",
      "[iter 200] loss=2.1910 val_loss=0.0000 scale=2.0000 norm=3.5209\n",
      "[iter 300] loss=1.9898 val_loss=0.0000 scale=1.0000 norm=1.6424\n",
      "[iter 400] loss=1.8316 val_loss=0.0000 scale=2.0000 norm=2.8945\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=2.6749 val_loss=0.0000 scale=2.0000 norm=4.8198\n",
      "[iter 200] loss=2.1963 val_loss=0.0000 scale=2.0000 norm=3.6718\n",
      "[iter 300] loss=1.9363 val_loss=0.0000 scale=2.0000 norm=3.0665\n",
      "[iter 400] loss=1.7396 val_loss=0.0000 scale=1.0000 norm=1.3021\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=2.7567 val_loss=0.0000 scale=2.0000 norm=5.8074\n",
      "[iter 200] loss=2.1881 val_loss=0.0000 scale=2.0000 norm=3.6162\n",
      "[iter 300] loss=1.9655 val_loss=0.0000 scale=1.0000 norm=1.6241\n",
      "[iter 400] loss=1.8181 val_loss=0.0000 scale=1.0000 norm=1.4710\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=1.6641 val_loss=0.0000 scale=1.0000 norm=1.3004\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=1.5364 val_loss=0.0000 scale=1.0000 norm=1.1615\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=1.6745 val_loss=0.0000 scale=2.0000 norm=2.6473\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=1.6263 val_loss=0.0000 scale=1.0000 norm=1.2541\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=1.5903 val_loss=0.0000 scale=1.0000 norm=1.2111\n",
      "[iter 0] loss=3.5953 val_loss=0.0000 scale=1.0000 norm=6.4366\n",
      "[iter 100] loss=1.6641 val_loss=0.0000 scale=1.0000 norm=1.3004\n",
      "[iter 200] loss=1.2698 val_loss=0.0000 scale=1.0000 norm=0.9971\n",
      "[iter 300] loss=1.0176 val_loss=0.0000 scale=1.0000 norm=0.8388\n",
      "[iter 400] loss=0.7883 val_loss=0.0000 scale=1.0000 norm=0.7238\n",
      "[iter 0] loss=3.6361 val_loss=0.0000 scale=1.0000 norm=6.7850\n",
      "[iter 100] loss=1.5364 val_loss=0.0000 scale=1.0000 norm=1.1615\n",
      "[iter 200] loss=1.1430 val_loss=0.0000 scale=0.5000 norm=0.4473\n",
      "[iter 300] loss=0.8248 val_loss=0.0000 scale=2.0000 norm=1.4713\n",
      "[iter 400] loss=0.6379 val_loss=0.0000 scale=1.0000 norm=0.6597\n",
      "[iter 0] loss=3.6349 val_loss=0.0000 scale=1.0000 norm=6.8405\n",
      "[iter 100] loss=1.6745 val_loss=0.0000 scale=2.0000 norm=2.6473\n",
      "[iter 200] loss=1.2768 val_loss=0.0000 scale=2.0000 norm=1.9891\n",
      "[iter 300] loss=0.9748 val_loss=0.0000 scale=1.0000 norm=0.8093\n",
      "[iter 400] loss=0.7415 val_loss=0.0000 scale=1.0000 norm=0.6999\n",
      "[iter 0] loss=3.5865 val_loss=0.0000 scale=1.0000 norm=6.3823\n",
      "[iter 100] loss=1.6263 val_loss=0.0000 scale=1.0000 norm=1.2541\n",
      "[iter 200] loss=1.2325 val_loss=0.0000 scale=2.0000 norm=1.8858\n",
      "[iter 300] loss=0.9653 val_loss=0.0000 scale=0.1250 norm=0.0988\n",
      "[iter 400] loss=0.7334 val_loss=0.0000 scale=0.5000 norm=0.3449\n",
      "[iter 0] loss=3.6444 val_loss=0.0000 scale=1.0000 norm=6.6872\n",
      "[iter 100] loss=1.5903 val_loss=0.0000 scale=1.0000 norm=1.2111\n",
      "[iter 200] loss=1.1709 val_loss=0.0000 scale=1.0000 norm=0.9215\n",
      "[iter 300] loss=0.8568 val_loss=0.0000 scale=0.5000 norm=0.3773\n",
      "[iter 400] loss=0.6168 val_loss=0.0000 scale=1.0000 norm=0.6417\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=1.7794 val_loss=0.0000 scale=1.0000 norm=1.4683\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=1.6671 val_loss=0.0000 scale=1.0000 norm=1.3056\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=1.7086 val_loss=0.0000 scale=1.0000 norm=1.3399\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=1.6674 val_loss=0.0000 scale=1.0000 norm=1.2786\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=1.8300 val_loss=0.0000 scale=1.0000 norm=1.5061\n",
      "[iter 0] loss=3.6238 val_loss=0.0000 scale=1.0000 norm=6.5984\n",
      "[iter 100] loss=1.7464 val_loss=0.0000 scale=1.0000 norm=1.4094\n",
      "[iter 200] loss=1.3530 val_loss=0.0000 scale=1.0000 norm=1.0644\n",
      "[iter 300] loss=1.1508 val_loss=0.0000 scale=1.0000 norm=0.9343\n",
      "[iter 400] loss=0.8479 val_loss=0.0000 scale=0.5000 norm=0.3719\n",
      "[iter 0] loss=3.6662 val_loss=0.0000 scale=1.0000 norm=7.0207\n",
      "[iter 100] loss=1.6872 val_loss=0.0000 scale=1.0000 norm=1.3309\n",
      "[iter 200] loss=1.3629 val_loss=0.0000 scale=1.0000 norm=1.0758\n",
      "[iter 300] loss=1.1200 val_loss=0.0000 scale=1.0000 norm=0.8856\n",
      "[iter 400] loss=0.8556 val_loss=0.0000 scale=1.0000 norm=0.7208\n",
      "[iter 0] loss=3.6933 val_loss=0.0000 scale=1.0000 norm=7.2347\n",
      "[iter 100] loss=1.7053 val_loss=0.0000 scale=1.0000 norm=1.3454\n",
      "[iter 200] loss=1.3693 val_loss=0.0000 scale=1.0000 norm=1.0905\n",
      "[iter 300] loss=1.1138 val_loss=0.0000 scale=1.0000 norm=0.9100\n",
      "[iter 400] loss=0.8489 val_loss=0.0000 scale=1.0000 norm=0.7647\n",
      "[iter 0] loss=3.6756 val_loss=0.0000 scale=1.0000 norm=6.8519\n",
      "[iter 100] loss=1.6747 val_loss=0.0000 scale=1.0000 norm=1.2878\n",
      "[iter 200] loss=1.3849 val_loss=0.0000 scale=1.0000 norm=1.0958\n",
      "[iter 300] loss=1.0620 val_loss=0.0000 scale=1.0000 norm=0.8409\n",
      "[iter 400] loss=0.7543 val_loss=0.0000 scale=1.0000 norm=0.6878\n",
      "[iter 0] loss=3.6839 val_loss=0.0000 scale=1.0000 norm=7.0080\n",
      "[iter 100] loss=1.8119 val_loss=0.0000 scale=1.0000 norm=1.4833\n",
      "[iter 200] loss=1.4019 val_loss=0.0000 scale=1.0000 norm=1.0882\n",
      "[iter 300] loss=1.0990 val_loss=0.0000 scale=1.0000 norm=0.8613\n",
      "[iter 400] loss=0.8652 val_loss=0.0000 scale=1.0000 norm=0.7282\n",
      "[iter 0] loss=3.6980 val_loss=0.0000 scale=1.0000 norm=6.9700\n",
      "[iter 100] loss=1.8131 val_loss=0.0000 scale=1.0000 norm=1.4459\n",
      "[iter 200] loss=1.4988 val_loss=0.0000 scale=1.0000 norm=1.1693\n",
      "[iter 300] loss=1.2173 val_loss=0.0000 scale=1.0000 norm=0.9748\n",
      "[iter 400] loss=1.0532 val_loss=0.0000 scale=1.0000 norm=0.8615\n",
      "NGBoost Best Test MSE: 6.917422985432078\n",
      "Best parameters: {'col_sample': 1.0, 'learning_rate': 0.05, 'minibatch_frac': 0.5, 'n_estimators': 500}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77316d760b4bbd62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
