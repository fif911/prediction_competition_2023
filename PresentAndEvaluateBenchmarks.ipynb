{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0573b9c2",
   "metadata": {},
   "source": [
    "# Presenting and evaluating benchmark models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4857d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import os\n",
    "\n",
    "# Evaluation scripts\n",
    "from CompetitionEvaluation import load_data, structure_data, calculate_metrics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find files\n",
    "username = os.getlogin()\n",
    "Mydropbox = f'/Users/{username}/Dropbox (ViEWS)/ViEWS/'\n",
    "overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Tables/'\n",
    "\n",
    "print('Dropbox path set to',Mydropbox)\n",
    "print('Overleaf path set to',overleafpath)\n",
    "\n",
    "filepath = Mydropbox + 'Prediction_competition_2023/' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bf775",
   "metadata": {},
   "source": [
    "## Reading in actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_actuals = pd.read_parquet(filepath + 'cm_actuals.parquet')\n",
    "df_pgm_actuals = pd.read_parquet(filepath + 'pgm_actuals.parquet')\n",
    "df_cm_actuals.tail(), df_pgm_actuals.head()\n",
    "# Recast to int32\n",
    "df_cm_actuals['ged_sb'] = df_cm_actuals['ged_sb'].astype('int32')\n",
    "df_pgm_actuals['ged_sb'] = df_pgm_actuals['ged_sb'].astype('int32')\n",
    "# Have to rename column name....:\n",
    "df_cm_actuals.rename(columns={\"ged_sb\": \"prediction\"}, errors=\"raise\", inplace=True)\n",
    "df_pgm_actuals.rename(columns={\"ged_sb\": \"prediction\"}, errors=\"raise\", inplace=True)\n",
    "# Summarize:\n",
    "print(df_cm_actuals.dtypes, df_pgm_actuals.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_actuals.describe(percentiles=[.25,.50,.75,.90,.95,.99,.992,.995])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d5877",
   "metadata": {},
   "source": [
    "## Reading in benchmark prediction models: \n",
    "\n",
    "Two models per level:\n",
    "\n",
    "1. cm model, based on ensemble\n",
    "2. cm model, based on historical values \n",
    "3. pgm model, based on ensemble\n",
    "4. pgm model, based on historical values\n",
    "\n",
    "Ã‹ach of these have predictions for each of four years; 2019, 2020, 2021, and 2022. The four years are collected in lists of dictionaries including dataframes and some metadata, one for each of the models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_actuals.query('country_id == 1')\n",
    "df_cm_actuals.loc[445:468]\n",
    "df_cm_actuals.head()\n",
    "df_cm_actuals.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_cm_ensemble = []\n",
    "bm_cm_historical_values = []\n",
    "bm_pgm_ensemble = []\n",
    "bm_pgm_historical_values = []\n",
    "\n",
    "def positive_integers(df, colname):\n",
    "    df[colname] = np.round(df[colname]).astype('int32')\n",
    "    df[colname][df[colname] < 0] = 0\n",
    "    return(df)\n",
    "\n",
    "colname = 'prediction'\n",
    "for year in [2018, 2019, 2020, 2021]:\n",
    "    print(year)\n",
    "    first_month = (year - 1980)*12 + 1\n",
    "    cm_e = {\n",
    "        'year': year,\n",
    "        'first_month': first_month,\n",
    "        'name': 'cm_ensemble',\n",
    "        'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_ensemble_' + str(year) + '.parquet'),colname),\n",
    "        'df_agg': pd.read_parquet(filepath + 'bm_cm_ensemble_agg' + str(year) + '.parquet'),\n",
    "        'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "    }\n",
    "    bm_cm_ensemble.append(cm_e)\n",
    "    cm_hv = {\n",
    "        'year': year,\n",
    "        'first_month': first_month,\n",
    "        'name': 'cm_historical_values',\n",
    "        'df_full': positive_integers(pd.read_parquet(filepath + 'bm_cm_historical_values_' + str(year) + '.parquet'),colname),\n",
    "        'df_agg': pd.read_parquet(filepath + 'bm_cm_historical_values_agg' + str(year) + '.parquet'),\n",
    "        'actuals': df_cm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "    }\n",
    "    bm_cm_historical_values.append(cm_hv)\n",
    "    if False:\n",
    "        pgm_e = {\n",
    "            'year': year,\n",
    "            'first_month': first_month,\n",
    "            'name': 'pgm_ensemble',\n",
    "            'df_full': positive_integers(pd.read_parquet(filepath + 'bm_pgm_ensemble_' + str(year) + '.parquet'),colname),\n",
    "            'df_agg': pd.read_parquet(filepath + 'bm_pgm_ensemble_agg' + str(year) + '.parquet'),\n",
    "            'actuals': df_pgm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "        }\n",
    "        bm_pgm_ensemble.append(pgm_e)\n",
    "        pgm_hv = {\n",
    "            'year': year,\n",
    "            'first_month': first_month,\n",
    "            'name': 'pgm_historical_values',\n",
    "            'df_full': positive_integers(pd.read_parquet(filepath + 'bm_pgm_historical_values_' + str(year) + '.parquet'),colname),\n",
    "            'df_agg': pd.read_parquet(filepath + 'bm_pgm_historical_values_agg' + str(year) + '.parquet'),\n",
    "            'actuals': df_pgm_actuals.loc[first_month: first_month + 12 - 1],\n",
    "        }\n",
    "        bm_pgm_historical_values.append(pgm_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether balanced panel:\n",
    "print(bm_cm_historical_values[0]['df_full'].head())\n",
    "print('Number of missing?',bm_cm_historical_values[0]['df_full'].isnull().sum())\n",
    "print(bm_cm_historical_values[0]['df_full'].describe())\n",
    "for m in range(445,457):\n",
    "    df = bm_cm_historical_values[0]['df_full'].loc[m]\n",
    "    print(m,len(df))\n",
    "    df2 =df.groupby([\"country_id\"]).agg({'prediction': [\"count\"]})\n",
    "    print(df2.describe())\n",
    "print(990*2292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bm_cm_historical_values[0]['df_full'].describe())\n",
    "print(bm_cm_historical_values[0]['df_full'].query('draw == 0').describe())\n",
    "print(bm_cm_historical_values[0]['df_full'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructuring, evaluating:\n",
    "\n",
    "# Evaluation parameters:\n",
    "ign_bins = [0, 0.5, 5, 25, 75, 150, 500, 1000, 10000]\n",
    "#ign_bins = [0, 0.5, 1000]\n",
    "\n",
    "\n",
    "for model_list in [bm_cm_ensemble, bm_cm_historical_values]:\n",
    "    for item in model_list:\n",
    "        print(item['name'], item['year'])\n",
    "        print(item['actuals'].describe())\n",
    "        print(item['df_full'].query('draw == 0').describe())\n",
    "        item['observed'], item['predictions'] = structure_data(item['actuals'], item['df_full']) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "        item['crps'] = calculate_metrics(item['observed'], item['predictions'], metric = 'crps') # calculates crps.\n",
    "#        item['ign'] = calculate_metrics(item['observed'], item['predictions'], metric = \"ign\", bins = ign_bins)\n",
    "\n",
    "        print('crps:',item['crps'])\n",
    "#        print('ignorance score:', item['ign'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(item['df_full']))\n",
    "print(len(item['df_full'])/12)\n",
    "print(len(item['df_full'])/(12*990))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17466c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IgnoranceScore import ensemble_ignorance_score, _ensemble_ignorance_score\n",
    "import numpy as np\n",
    "observations = [0, 1, 50, 500]\n",
    "forecasts = np.array([[0, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 2, 55],\n",
    "                      [500, 49, 52, 52, 500],\n",
    "                      [49, 49, 49, 49, 500]])\n",
    "bins = [0, 0.5, 10.5, 50.5, 100.5, 1000.5]\n",
    "res = ensemble_ignorance_score(observations, forecasts, prob_type=3, ign_max=None, round_values=False, axis=-1, bins = bins, low_bin=0, high_bin=1000)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CompetitionEvaluation import load_data, structure_data, calculate_metrics\n",
    " \n",
    "observed, predictions = load_data(forecasts_path=filepath + \"cm_benchmark_ensemble_550.parquet\",\n",
    "                                    observed_path=filepath + \"cm_actuals.parquet\")\n",
    "predictions[\"prediction\"] = predictions[\"prediction\"].replace(-1, 0)\n",
    "observed, predictions = structure_data(observed, predictions)\n",
    "metrics = calculate_metrics(observed, predictions, metric = \"ign\", round_values = True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(calculate_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7781c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "#observed, predictions = load_data(args.o, args.p) # read parquet files to pandas\n",
    "observed, predictions = structure_data(df_pgm_actuals, df_bm_pgm_historical_values) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "metrics = calculate_metrics(observed, predictions) # calculates crps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_cm_ensemble = pd.read_parquet(filepath + 'cm_benchmark_ensemble_550.parquet')\n",
    "df_bm_cm_ensemble.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd111b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_cm_ensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805136d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49577f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_pgm_historical_values = pd.read_parquet(filepath + 'pgm_benchmark_historical_values_step_3.parquet')\n",
    "df_bm_pgm_historical_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observed, predictions = load_data(args.o, args.p) # read parquet files to pandas\n",
    "observed, predictions = structure_data(df_cm_actuals, df_bm_cm_ensemble) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "metrics = calculate_metrics(observed, predictions) # calculates crps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in for all 12 steps\n",
    "from datetime import datetime\n",
    "print(\"Cell started to run:\", datetime.now())\n",
    "\n",
    "df_pgm_hv = []\n",
    "for step in range(3,14+1):\n",
    "    df = pd.read_parquet(filepath + 'pgm_benchmark_historical_values_step_' + str(step) + '.parquet')\n",
    "    print(step, df.describe())\n",
    "    df_pgm_hv.append(df)\n",
    "    \n",
    "print(\"Cell run ended:\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cell started to run:\", datetime.now())\n",
    "i = 3\n",
    "for df in df_pgm_hv:\n",
    "    print('step',i,datetime.now())\n",
    "    observed, predictions = structure_data(df_pgm_actuals, df) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "    metrics = calculate_metrics(observed, predictions) # calculates crps.\n",
    "    print(metrics)\n",
    "    i=i+1\n",
    "print(\"Cell run ended:\", datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b41a7",
   "metadata": {},
   "source": [
    "# Read in the sc-type prediction files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm_pgm_ensemble2022 = pd.read_parquet(filepath + 'bm_pgm_ensemble_2022.parquet')\n",
    "df_pgm_actuals_2022 = df_pgm_actuals.loc[505:516]\n",
    "df_bm_pgm_ensemble2022.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observed, predictions = structure_data(df_pgm_actuals_2022, df_bm_pgm_ensemble2022) # structure data as xarrays that the xskillscore.crps_ensemble wants\n",
    "metrics = calculate_metrics(observed, predictions) # calculates crps.\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65dc5cc",
   "metadata": {},
   "source": [
    "# Creating samples based on point predictions\n",
    "\n",
    "Assuming Poisson distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_ensemble_aggregated = pd.read_parquet(filepath + 'cm_benchmark_ensemble_550_aggregated.parquet')\n",
    "\n",
    "print(cm_ensemble_aggregated.describe())\n",
    "print(cm_ensemble_aggregated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip down to a year of sc predictions:\n",
    "df_cm_ensemble = []\n",
    "for step in range(3,14+1):\n",
    "    df = cm_ensemble_aggregated['mean_log_prediction'].loc[442+step]\n",
    "    df = pd.DataFrame(df[df.index.get_level_values('step').isin([step])])\n",
    "    df['prediction'] = np.expm1(df['mean_log_prediction'])\n",
    "    df_cm_ensemble.append(df)\n",
    "\n",
    "df_cm_ensemble_stripped = pd.concat(df_cm_ensemble)\n",
    "print(df_cm_ensemble_stripped.describe())\n",
    "print(df_cm_ensemble_stripped.tail(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
